{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to data parsing.\n",
    "\n",
    "This notebook parser the data from the video tracking files into image folders with there respective labels. It does this into the following format.\n",
    "```\n",
    "../data/fish_tracking\n",
    "├── images\n",
    "├── labels\n",
    "├── test_per_vid.txt\n",
    "├── train_per_vid.txt\n",
    "└── val_per_vid.txt\n",
    "```\n",
    "Where images and label folder contain the images and label with the same respective name. The text files contain the images names for the respective split. \n",
    "The current split is made on seperating videos per video folder. Each folder contains 3 camera angles and are considered to be in the same split. The annotation currently has a point annotation on the head. To create a bounding box around this point we add a radius and define a box from this. This is far from an ideal approach and should be revised in the future. The current data parser only parser the predator fish, in the future the prey could be added by changing the data loader. The reason for only selecting the predator is due to the narrowing of the task to predator first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "# Native\n",
    "import os\n",
    "import pickle\n",
    "from datetime import date\n",
    "\n",
    "# 3th party\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from progressbar import progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config variables are defined in the next cell. Please adapt in needed, however please check the influence of your adaption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General setting\n",
    "today = date.today()\n",
    "today = today.strftime('%Y_%m_%d')\n",
    "\n",
    "\n",
    "# Directory settings\n",
    "dir = \"../data/tracked_fish_3_camera_angles\"\n",
    "destination_dir = f\"../data/{today}_fish_tracking\"\n",
    "video_dir = \"tracking_vids\"\n",
    "label_dir = \"h5_tracking_files\"\n",
    "camera_angles = [\"A\",\"B\",\"C\"]\n",
    "\n",
    "# Predator = 0\n",
    "# Prey = 1\n",
    "#classes = [\"0\", \"1\"]\n",
    "\n",
    "# annotation setting\n",
    "radius = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "\n",
    "dest_img_path = os.path.join(destination_dir, \"images\")\n",
    "dest_label_path = os.path.join(destination_dir, \"labels\")\n",
    "\n",
    "# Create directories if not exists\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)\n",
    "    os.makedirs(dest_img_path)\n",
    "    os.makedirs(dest_label_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load loading and parsing.\n",
    "This loops over the respective video and parses them in the image and label directory. This does not include the splitting because this can be subject to change. Currently the labeling is done in a coarse format and is not able to be directly used by the mmdetection framework. A future task would be to directly change this to the coco format. In this loop currently only the predator is parsed, if one would like to parse the prey and extra loop would need to be made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (36 of 36) |########################| Elapsed Time: 0:08:55 Time:  0:08:55\n"
     ]
    }
   ],
   "source": [
    "# Loop over all subdir\n",
    "for d in progressbar(sorted(os.listdir(dir))):\n",
    "\n",
    "    # Define paths\n",
    "    sub_d = os.path.join(dir, d)\n",
    "    video_path = os.path.join(sub_d, video_dir)\n",
    "    annotation_path = os.path.join(sub_d, label_dir)\n",
    "    \n",
    "    # loop over different angles\n",
    "    for angle in camera_angles:\n",
    "\n",
    "        # Load annotation file\n",
    "        annotation_file = os.path.join(annotation_path, f\"{d}Corr{angle}.pkl\")\n",
    "        annotation_dict = pickle.load(open(annotation_file, \"rb\"))\n",
    "        \n",
    "        # Load video\n",
    "        video_file = os.path.join(video_path, f\"{d}_{angle}.mp4\")\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        \n",
    "        # Loop over frames with annotation.\n",
    "        # TODO: Need to be extended later on for prey\n",
    "        i = 0\n",
    "        for x,y in zip(annotation_dict[\"0\"][\"X\"],annotation_dict[\"0\"][\"Y\"]):\n",
    "            \n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if ~np.isnan(x) & ~np.isnan(y):\n",
    "                # save frame\n",
    "                \n",
    "                # Center coordinates is a single map_images folder but not directly align with the annotations.  The current approach is \n",
    "                center_coordinates = (int(x), int(y))\n",
    "                \n",
    "                # Determine bbox\n",
    "                xmin = int(x) - radius\n",
    "                ymin = int(y) - radius\n",
    "                xmax = int(x) + radius\n",
    "                ymax = int(y) + radius\n",
    "\n",
    "                # Get frame size\n",
    "                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH ))\n",
    "                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT ))\n",
    "                # fps =  cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "                image = frame\n",
    "\n",
    "                # UNCOMMENT FOR SANITY CHECK\n",
    "                # Blue color in BGR\n",
    "                # color = (0, 255, 0)\n",
    "                \n",
    "                # Line thickness of 2 px\n",
    "                # thickness = 2\n",
    "\n",
    "                # Write onto frame (sanity check)\n",
    "                # image = cv2.circle(image, center_coordinates, radius, color, thickness)\n",
    "                \n",
    "                # Naming scheme\n",
    "                idx = i\n",
    "                idx = str(idx)\n",
    "                idx = idx.zfill(5) \n",
    "                image_name = f\"{d}_{angle}_{idx}\"\n",
    "\n",
    "                # Add img to list for data splitting\n",
    "                img_list.append(image_name)\n",
    "                \n",
    "                # Write img\n",
    "                # print(image_name)\n",
    "                cv2.imwrite(f\"{dest_img_path}/{image_name}.jpg\", image)\n",
    "                \n",
    "                # write label\n",
    "                label = [\"predator\", \"0\", xmin, ymin, xmax, ymax, width, height]\n",
    "                with open(f'{dest_label_path}/{image_name}.txt', 'w') as f:\n",
    "                    for item in label:\n",
    "                        f.write(\"%s \" % item)\n",
    "                \n",
    "                i+= 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training videos: ['325', '461', '131', '447', '453', '238', '204', '494', '413', '148', '248', '451', '297', '456', '239', '293', '243', '252', '306', '458', '419', '449']\n",
      "Validation videos: ['283', '242', '144', '343', '373', '463']\n",
      "Testing videos: ['457', '469', '455', '160', '400', '352', '417', '442']\n"
     ]
    }
   ],
   "source": [
    "vid_list = sorted(os.listdir(dir))\n",
    "    \n",
    "train, test = train_test_split(vid_list, test_size=0.2, random_state=1)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=1)\n",
    "\n",
    "print(f\"Training videos: {train}\")\n",
    "print(f\"Validation videos: {val}\")\n",
    "print(f\"Testing videos: {test}\")\n",
    "\n",
    "def create_anno_file(data, data_dir, set_list, anno_file): \n",
    "    with open(f'{data_dir}/{anno_file}.txt', 'w') as f:\n",
    "        for t in sorted(set_list):\n",
    "            for d in sorted(os.listdir(data)):\n",
    "                if t == d.split('_')[0]:\n",
    "                    f.write(\"%s\\n\" % os.path.splitext(d)[0])\n",
    "\n",
    "create_anno_file(dest_img_path, destination_dir, train, \"train\")\n",
    "create_anno_file(dest_img_path, destination_dir, val, \"val\")\n",
    "create_anno_file(dest_img_path, destination_dir, test, \"test\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2004f4c0fdc6126308f60ae3c58c86001150d3e3fc5aebba234bf354c225f14e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pytorch_cuda_11.2': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
